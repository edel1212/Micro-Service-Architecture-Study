# Kafka
- Docker Compose ì‚¬ìš©í•˜ì—¬ ì§„í–‰
```yaml
services:
  zookeeper:
    image: bitnami/zookeeper:latest
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"
  kafka:
    image: bitnami/kafka:latest 
    container_name: kafka
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
    ports:
      - "9092:9092"
```

## 1 ) ê¸°ë³¸ ëª…ë ¹ì–´

### 1 - 1 ) ë©”ì„¸ì§€ ìƒì„±
- ìµœì‹  ë²„ì „(Kafka 2.0 ì´ìƒ)ì—ì„œëŠ” `--broker-list` ëŒ€ì‹  `--bootstrap-server`ë¥¼ **ì‚¬ìš©í•´ì•¼ í•¨**
  - Kafkaì—ì„œ í´ë¼ì´ì–¸íŠ¸(í”„ë¡œë“€ì„œ/ì»¨ìŠˆë¨¸)ê°€ Kafka **ë¸Œë¡œì»¤(ì„œë²„)ì™€ ì—°ê²°í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì˜µì…˜**
    - Kafka í´ëŸ¬ìŠ¤í„°ì— ìˆëŠ” **í•˜ë‚˜ ì´ìƒì˜ ë¸Œë¡œì»¤(Broker) ì£¼ì†Œë¥¼ ì§€ì •í•´ì„œ í´ë¼ì´ì–¸íŠ¸ê°€ í´ëŸ¬ìŠ¤í„°ì— ì ‘ì†**í•  ìˆ˜ ìˆë„ë¡ í•¨
```shell
kafka-topics.sh --create --topic [ ìƒì„±í•  topicëª… ] --bootstrap-server [ Kafka Broker ë„ë©”ì¸ ]
```

### 1 - 2 ) í”„ë¡œë“€ìŠ¤ ëª¨ë“œ ì§„ì… ë° ë©”ì„¸ì§€ ë°œì†¡
```shell
kafka-console-producer.sh --bootstrap-server [ Kafka Broker ë„ë©”ì¸ ] --topic [ ìƒì„± í–ˆë˜ topicëª… ]
# kafka-console-producer.sh --bootstrap-server 127.0.01:9092 --topic test
```

## 2 ) Kafka Connect
> ì¹´í”„ì¹´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ ë°ì´í„°ë¥¼ ì£¼ê³  ë°›ê¸° ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œ
- ë°ì´í„°ë² ì´ìŠ¤, í‚¤-ê°’ ì €ì¥ì†Œ, ê²€ìƒ‰ ì¸ë±ìŠ¤ ë° íŒŒì¼ ì‹œìŠ¤í…œ ê°„ì˜ **ê°„ë‹¨í•œ ë°ì´í„° í†µí•©ì„ ìœ„í•œ ì¤‘ì•™ ì§‘ì¤‘ì‹ ë°ì´í„° í—ˆë¸Œ ì—­í• ** í•œë‹¤.
  -  Kafkaì™€ ë‹¤ë¥¸ ë°ì´í„° ì‹œìŠ¤í…œ ê°„ì— ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ê³  Kafka ì•ˆíŒìœ¼ë¡œ ëŒ€ê·œëª¨ ë°ì´í„° ì…‹ì„ ì´ë™ì‹œì¼œì£¼ëŠ” ì»¤ë„¥í„°ë¥¼ ë¹ ë¥´ê²Œ ìƒì„± ê°€ëŠ¥
- í”„ë¡œë“€ì„œì™€ ì»¨ìŠˆë¨¸ë¥¼ ì§ì ‘ ê°œë°œí•´ ì›í•˜ëŠ” ë™ì‘ì„ ì‹¤í–‰í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆì§€ë§Œ, ê°œë°œí•˜ê³  ìš´ì˜í•˜ëŠ”ë° ë“¤ì–´ê°€ëŠ” ë¦¬ì†ŒìŠ¤ë‚˜ ë¹„ìš©ì´ ë¶€ë‹´ì´ ë˜ëŠ” ê²½ìš° ì¹´í”„ì¹´ ì»¤ë„¥íŠ¸ë¥¼ ì‚¬ìš©
  - **ì¹´í”„ì¹´ ì»¤ë„¥íŠ¸ì—ì„œ ì œê³µí•˜ëŠ” REST APIë¥¼ í†µí•´** ë¹ ë¥´ê³  ê°„ë‹¨í•˜ê²Œ ì»¤ë„¥íŠ¸ì˜ ì„¤ì •ì„ ì¡°ì •í•˜ë©° ìƒí™©ì— ë§ê²Œ **ìœ ì—°í•˜ê²Œ ëŒ€ì‘ ê°€ëŠ¥**

### 2 - 1 ) Kafka Connect Docker
- Volume ì„¤ì • ì‹œ í•„ìš”ë¡œ í•˜ëŠ” ìë£Œë¥¼ ë¯¸ë¦¬ ì¤€ë¹„í•´ì•¼í•œë‹¤.
  - Mariadb Client ( gradle or mavenì—ì„œ ë°›ì€ jarë¥¼ ë³µì‚¬ í›„ ì‚¬ìš© )
  - confluentinc Jdbc lib ( ê³µì‹ í™ˆí˜ì´ì§€ì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ lib ìœ„ì¹˜ jar ì‚¬ìš© )
- â˜ ï¸ ì‚½ì§ˆ
  - ë²„ì „ í˜¸í™˜ ë¬¸ì œ
    - kafka ë° kafka-connect **ë²„ì „ ë‹¤ìš´ê·¸ë ˆì´ë“œ**
  > The issue was resolved by downgrading Kafka to version 3.8.x. According to the release notes for Kafka 3.9.0, the SystemTime class was changed to a singleton,   
  > and instead of creating an instance with new SystemTime();, it was modified to obtain the instance by calling the SystemTime.getSystemTime(); method.
  - kafka ì™¸ë¶€ ì ‘ê·¼ ë¶ˆê°€ ë¬¸ì œ
    - KAFKA_CFG_LISTENERS - ì„¤ì • í•„ìš”
    - KAFKA_CFG_ADVERTISED_LISTENERS - ì„¤ì • í•„ìš”ì •
  - `KAFKA_CFG` Prefix
    - bitnamiìš© Kafka Docker Imageë¥¼ ì‚¬ìš©í•  ê²½ìš° ê¶Œì¥í•˜ëŠ” ë°©ë²•ì´ë‹¤.
  - EXTERNAL ì„¤ì •
    - kafkaê°€ ë‚´ë¶€, ì™¸ë¶€ ì„¤ì •ì´ í•„ìš”í•  ê²½ìš° ì„¤ì •í•´ì¤˜ì•¼í•œë‹¤.
```yaml
services:
  zookeeper:
    image: bitnami/zookeeper:latest
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"
    networks:
      - ecommerce-network

  kafka:
    image: bitnami/kafka:3.8.0  # âœ… kafka connector JDBC ì´ìŠˆë¡œì¸í•œ ë²„ì „ ë‹¤ìš´
    container_name: kafka
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:19092,EXTERNAL://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
    ports:
      - 9092:9092
      - 19092:19092
    networks:
      - ecommerce-network

  kafka-connector-mariadb:
    image: confluentinc/cp-kafka-connect:7.7.0 # âœ… í˜¸í™˜ ë¬¸ì œë¡œ ì¸í•œ ë²„ì „ ë‹¤ìš´
    container_name: kafka-connector
    ports:
      - 8083:8083
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:19092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "quickstart-avro"
      CONNECT_CONFIG_STORAGE_TOPIC: "quickstart-avro-config"
      CONNECT_OFFSET_STORAGE_TOPIC: "quickstart-avro-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "quickstart-avro-status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_REST_ADVERTISED_HOST_NAME: "localhost"
      CONNECT_LOG4J_ROOT_LOGLEVEL: DEBUG
      CONNECT_PLUGIN_PATH: "/usr/share/java,/etc/kafka-connect/jars"
    volumes:
      - ./jars:/etc/kafka-connect/jars #jarsíŒŒì¼ë“¤ volumeì„ í†µí•˜ì—¬ ì‚¬ìš©
    networks:
      - ecommerce-network
    depends_on:
      - kafka

networks:
  ecommerce-network:
    driver: bridge
```

### 2 - 2 ) Kafka Source Connect
- ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ì‹œìŠ¤í…œì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ Kafka í† í”½ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì†¡í•˜ëŠ” ì—­í• ì„ í•˜ëŠ” Kafka Connectì˜ êµ¬ì„± ìš”ì†Œ
  - ë°ì´í„°ë² ì´ìŠ¤, íŒŒì¼ ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ Kafkaë¡œ ë°ì´í„°ë¥¼ ì‰½ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
  
#### 2 - 2 - A ) Source Connect ë“±ë¡ 
```yaml
# â˜ ï¸ ì‚½ì§ˆ : config.connector.class ì„¤ì • ì‹œ "JdbcSourceConnector"ë¡œ í•´ì¤˜ì•¼í•œë‹¤.. 
#         - "jdbc.JdbcSinkConnector" ë¡œ ì„¤ì •í•˜ì—¬ ì‚½ì§ˆí•¨ ì‚¬ìš©ì²˜ê°€ ë‹¤ë¦„ ... ( ì—ëŸ¬ê°€ ì—†ì–´ ë” ì°¾ê¸° ì˜¤ë˜ ê±¸ë¦¼ )
#         - JdbcSourceConnector : ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ Kafkaë¡œ ë°ì´í„° ì½ê¸° | JdbcSinkConnector : Kafkaì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë°ì´í„° ì“°ê¸°
```
- Http Method : POST
- Header : Content-Type - application/json
- Uri : `localhost:8083/connectors`
- Body
```javascript
{
    "name": "my-source-connect",
    "config": {
        "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
        "connection.url": "jdbc:mariadb://local-db:3306/mydb",
        "connection.user": "root",
        "connection.password": "123",
        "mode": "incrementing",
        "incrementing.column.name": "id",
        "table.whitelist": "users",
        "topic.prefix": "my_topic_",
        "tasks.max": "1",
        "poll.interval.ms" : 10000,
        "topic.creation.default.replication.factor":1,
        "topic.creation.default.partitions" : 1
    }
}
```

#### 2 - 2 - B ) Connect ëª©ë¡ í™•ì¸ 
- Http Method : GET
- Uri : `localhost:8083/connectors/list`

#### 2 - 2 - C ) ì§€ì • Connect ìƒíƒœ í™•ì¸
- Http Method : GET
- Uri : `localhost:8083/connectors/{{ì§€ì • sourse name}}/status`
- 
#### 2 - 2 - D ) Source Connect í…ŒìŠ¤íŠ¸
- 1 . kafka consumer.shì—ì„œ ì„¤ì •í•œ topic ë‚´ìš© ë³´ê¸°
  - `kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic {{ì§€ì • í† í”½}} --from--beginning`
- 2 . connectorë¡œ ì§€ì •í•œ Tableì— Data Insert
  - `insert into users(user_id, name) values("foo","jeoungho")`
- 3 . topic log í™•ì¸


### 2 - 3 ) Kafka Sink Connect
- Kafka í† í”½ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ ì™¸ë¶€ ì‹œìŠ¤í…œì´ë‚˜ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ì „ì†¡í•˜ëŠ” ì—­í• ì„ í•˜ëŠ” Kafka Connectì˜ êµ¬ì„±
  - Kafkaì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ë‹¤ì–‘í•œ ëª©ì ì§€ ì‹œìŠ¤í…œ, ì˜ˆë¥¼ ë“¤ì–´ ë°ì´í„°ë² ì´ìŠ¤, íŒŒì¼ ì‹œìŠ¤í…œ, ê²€ìƒ‰ ì—”ì§„ ë“±ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ì €ì¥í•˜ê±°ë‚˜ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ

#### 2 - 3 - A ) Sink Connect ë“±ë¡
```yaml
# âœ… JdbcSinkConnector ë¥¼ ë“±ë¡ í•´ì•¼í•œë‹¤.
#    - "topic" -> "topics" ë¡œ ë“±ë¡í•´ì•¼í•œë‹¤. 
#      - sink ë“±ë¡ ì‹œ í•´ë‹¹ topicsë¡œ ì§€ì •ëœ "Tableì´ ìƒì„±"ëœë‹¤.
#
# > ì§€ì •í•œ topicsì¸ "my_topic_users" í…Œì´ë¸”ì´ ìƒì„±ëœë‹¤. 
#   - source connectorì— ë“±ë¡ëœ topicì„ sink connectorë¡œ ê°ì§€í•´ì„œ í•´ë‹¹ ê°’ì„ ê·¸ëŒ€ë¡œ tableë¡œ ë™ê¸°í™” í•´ì£¼ëŠ” ê²ƒì´ë‹¤
```
- Http Method : POST
- Header : Content-Type - application/json
- Uri : `localhost:8083/connectors`
- Body
```javascript
{
  "name": "my-sink-connect",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector", 
    "connection.url": "jdbc:mariadb://local-db:3306/mydb",
    "connection.user": "root",
    "connection.password": "123",
    "auto.create": "true",
    "auto.evolve": "true",
    "delete.enabled": "false",
    "tasks.max": "1",
    "topics": "my_topic_users"  
  }
}
```

#### 2 - 3 - B ) Sink Connect í…ŒìŠ¤íŠ¸
- 1 . sink connectorë¡œ ìƒì„±ëœ Tableì— ì¡°íšŒ
  - source connector ì§€ì • tableê³¼ ë™ê¸°í™” ë˜ì–´ìˆëŠ”ê²ƒì„ í™•ì¸ ê°€ëŠ¥  
- 2 . source connector ì§€ì • tableì— data insert
  - source connector ì§€ì • tableê³¼ ë™ê¸°í™” ë˜ì–´ìˆëŠ”ê²ƒì„ í™•ì¸ ê°€ëŠ¥
- 3 . kafka-console-consumer.sh ì—ì„œ í•´ë‹¹ topic logë¥¼ ì‚¬ìš©í•´ì„œ `kafka-console-producer`ì— ì£¼ì…
  ```javascript
  {
    "type": "string",
    "optional": true,
    "field": "user_id"
  },
  {
    "type": "string",
    "optional": true,
    "field": "pwd"
  },
  {
    "type": "string",
    "optional": true,
    "field": "name"
  },
  {
    "type": "int64",
    "optional": true,
    "name": "org.apache.kafka.connect.data.Timestamp",
    "version": 1,
    "field": "created_at"
  }
  ],
  "optional": false,
  "name": "users"
  },
  "payload": {
    "id": 9,
    "user_id": "add-userd-producer",
    "pwd": null,
    "name": "kafka-producer",
    "created_at": 1741437467000
  }
  ```
- 4 . sink connectorë¡œ ìƒì„±ëœ Tableì— ì¡°íšŒ ì‹œ í•´ë‹¹ ê°’ì´ ì¶”ê°€ ë˜ì–´ ìˆìŒ
  - âœ… **sink tableì—ëŠ” ì¶”ê°€ë˜ì–´ ìˆì§€ë§Œ source tableì—ëŠ” ì¶”ê°€ ë˜ì–´ ìˆì§€ ì•ŠìŒ** => ë‹¹ì—°í•œ ê²°ê³¼ì´ë‹¤. 
    - sink table : kafkaì˜ topicì—ì„œ ë„˜ì–´ì˜¨ **JSON ê¸°ì¤€ìœ¼ë¡œ Table ë°ì´í„° ìƒì„±**
    - source connector ì§€ì§• table : ì›ë³¸ í…Œì´ë¸”

## 3 ) Kafka using spring-boot

### 3 - 1 ) kafka consumer

#### 3 - 1 - A ) build.gradle
```groovy
dependencies {
  // kafka
  implementation 'org.springframework.kafka:spring-kafka'
  testImplementation 'org.springframework.kafka:spring-kafka-test'
}
```

#### 3 - 1 - B ) kafka consumer config
- `@EnableKafka`ë¥¼ ì‚¬ìš©í•˜ì—¬ KafkaListener í™œì„±í™”
  - **Kafka Listenerë¥¼ ìë™ìœ¼ë¡œ ê°ì§€**í•˜ê³  **ê´€ë¦¬**í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•˜ëŠ” ì—­í• ì„ í•¨
- ì‚¬ìš©í•˜ê¸°ì— StringDeserializer **ì„¤ì • í•„ìˆ˜**
```java
@EnableKafka
@Configuration
public class KafkaConsumerConfig {
    @Bean
    public ConsumerFactory<String, String> consumerFactory(){
        Map<String, Object> properties = new HashMap<>();
        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "127.0.0.1:9092");
        properties.put(ConsumerConfig.GROUP_ID_CONFIG, "consumerGroupId");
        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

        return new DefaultKafkaConsumerFactory<>(properties);
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory
                = new ConcurrentKafkaListenerContainerFactory<>();
        kafkaListenerContainerFactory.setConsumerFactory(consumerFactory());

        return kafkaListenerContainerFactory;
    }
}
```
#### 3 - 1 - C ) kafka consumer (Message Receive)
- `@KafkaListener(topics = "ì „ë‹¬ë°›ì„ í† í”½ ì§€ì •")`ì„ ì§€ì •í•œ ë©”ì„œë“œì˜ argumentì—ì„œ messageë¥¼ ë°›ìŒ
```java
@Service
@RequiredArgsConstructor
@Log4j2
public class KafkaConsumer {
    private final CatalogRepository catalogRepository;
    private final ObjectMapper mapper;

    @KafkaListener(topics = "example-catalog-topic")
    public void updateQty(String kafkaMessage){
        log.info("kafka Message: -> {}", kafkaMessage);
        Map<Object, Object> map = new HashMap<>();
        // String -> JSON
        try {
            map = mapper.readValue(kafkaMessage, new TypeReference<>() {});
        } catch (Exception e){
            e.printStackTrace();;
        } // try - catch

        String productId = (String) map.get("productId");
        log.info("productId :: {}", productId);

        CatalogEntity entity = catalogRepository.findByProductId(productId);
        if(entity != null){
            // update to qty
            int qty = (Integer) map.get("qty");
            entity.setStock(entity.getStock() - qty);
            catalogRepository.save(entity);
        } // if

    }
}
```

### 3 - 2 ) kafka producer
- build.gradleì€ ê°™ê¸°ì— ì œì™¸

#### 3 - 2 - A ) kafka producer config
- ì§ë ¬ì™€ë¥¼ ìœ„í•´ `StringSerializer`ë¥¼ ì‚¬ìš©í•´ì•¼í•¨
  - âœ… ì£¼ì˜ì‚¬í•­ : import ì‹œ javaì—ì„œ ì œê³µí•˜ëŠ” classê°€ ì•„ë‹Œ **kafkaì—ì„œ ì œê³µí•˜ëŠ” class ì‚¬ìš©**
```java
@EnableKafka
@Configuration
public class KafkaProducerConfig {
    @Bean
    public ProducerFactory<String, String> producerFactory(){
        Map<String, Object> properties = new HashMap<>();
        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "127.0.0.1:9092");
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);

        return new DefaultKafkaProducerFactory<>(properties);
    }

    @Bean
    public KafkaTemplate<String, String> kafkaTemplate(){
        return new KafkaTemplate<>(producerFactory());
    }
}
```

#### 3 - 2 - B ) kafka producer (Message send)
```java
@Log4j2
@Service
@RequiredArgsConstructor
public class KafkaProducer {
    private final KafkaTemplate<String, String> kafkaTemplate;
    private final ObjectMapper objectMapper;

    public OrderDto send(String topic, OrderDto orderDto){
        String jsonInString = "";
        try{
            jsonInString = objectMapper.writeValueAsString(orderDto);
        } catch (Exception e){
            e.printStackTrace();
        } // try - catch

        kafkaTemplate.send(topic, jsonInString);
        log.info("kafka Producer sent data from the Order micro service :: {}", orderDto);

        return orderDto;
    }
}
```

## 4 ) Kafka Connect using spring-boot
```yaml
# âœ… ì „ì²´ì ì¸ íë¦„
#   - ğŸ‘‰ ì‚¬ìš©ìì˜ ì£¼ë¬¸ ìš”ì²­
#   - [ Producer ] Order Service Application ( Nê°œ )
#                  - Kafkaì˜ ì§€ì • í† í”½ì¸ "example-catalog-topic"ì„ í†µí—¤ Message ì „ë‹¬
#                  - Kafka Connector Sinkë¥¼ ì‚¬ìš© DBì €ì¥
#   - [ Consumer ] Catalog Service Application ( 1ê°œ )
#                  - "example-catalog-topic"ì„ í†µí—¤ Message ì „ë‹¬ ë°›ì€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ Catalog DB ìˆ˜ì •
```

### 4 - 1 ) Kafka Sink Connect ì„¤ì •
- ì´ë¯¸ ìƒì„±ë˜ì–´ ìˆëŠ” DBì— Kafka Producerë¥¼ í†µí•´ Messageë¥¼ ì£¼ì…í•˜ì—¬ Dataë¥¼ Insert í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ **Kafka Source Connect ì„¤ì • ë¶ˆí•„ìš”**
  - Kafka Source Connectì€ **ê°ì§€ ëŒ€ìƒì˜ ë³€ê²½ëœ ë‚´ìš©ì„ kafka Messageë¡œ ë°€ì–´ë„£ëŠ” ë°©ë²•**ì´ê¸°ì— ì„¤ì •ì´ ë¶ˆí•„ìš” í•œê²ƒ
- topicsì˜ ì´ë¦„ì€ **ëŒ€ìƒì´ ë  Tableëª…**ìœ¼ë¡œ ì§€ì •
```javascript
{
  "name": "my-order-sink-connect",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector", 
    "connection.url": "jdbc:mariadb://local-db:3306/mydb",
    "connection.user": "root",
    "connection.password": "123",
    "auto.create": "true",
    "auto.evolve": "true",
    "delete.enabled": "false",
    "tasks.max": "1",
    // ğŸ‘‰ Tableëª…ìœ¼ë¡œ ì§€ì •
    "topics": "orders"  
  }
}
```
